---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍如何通过 jdbc 的方式在 Hive on Spark 中访问外部数据源。

#### 实验环境
 
当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* JDK version "1.8.0_172"
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* Spark version: 2.4.3
* IntelliJ IDEA Community Version: 2019.3.4

#### 知识点

在 Hive on Spark 中支持 SQL 方式关联外部数据源，只要在 Spark 中创建了外部数据源的关联表，就可以按照 JDBC 标准通过 Spark 访问外部数据源。

在 Spark 中创建关联表只需要指定数据源，无需定义表结构，在关联时 Spark 会根据结构化数据源自动生成表结构。以关联 SequoiaDB 集合空间为例，创建关联表语法及参数说明如下：

![1738-420-驱动](https://doc.shiyanlou.com/courses/1738/1207281/a040460c8cb09d8a2758b94dc284e93d-0)

> **说明**
>
> com.sequoiadb.spark 为巨杉数据库开发的 Spark 连接器，可以在 Spark 创建表时指定和 SequoiaDB 中的集合空间关联并和集合的各个数据分区做好对接。
>
> 使用连接器需要确保 `$SPARK_HOME` 的 `jars` 目录下有 `spark-sequoiadb` jar 包，并在 `spark-env.sh` 文件中配置 SPARK_CLASSPATH。在启动 thriftserver 时会自动识别 jar 包并在提交 SQL 到 Hive on Spark 时调用。

## 打开项目
 
#### 打开 IDEA

打开 IDEA 代码开发工具。

![1738-420-01](https://doc.shiyanlou.com/courses/1738/1207281/b478b77e961e05b3f8bfa0bf327a58ad-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-420-02](https://doc.shiyanlou.com/courses/1738/1207281/738ff5ff514f299a6dcfaf7340367c2f-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-420-03](https://doc.shiyanlou.com/courses/1738/1207281/5b3f78991e52edba44b04bfc3324bf31-0)

#### Maven 依赖

如图所示找到 pom.xml 文件：

![1738-410-pom](https://doc.shiyanlou.com/courses/1738/1207281/2096e77f8ff05283b1b51e9f5182b861-0)

在 pom.xml 文件中可以找到当前实验中使用到的 Maven 依赖：

![1738-420-04](https://doc.shiyanlou.com/courses/1738/1207281/d2169f36c0d88c23f13ac644f3055eba-0)

## 关联 SequoiaDB 集合

程序将通过 SequoiaSQL-MySQL 实例初始化 employee 表存储到 SequoiaDB 中，通过 Hive on Spark 建立和 SequoiaDB 的 employee 集合关联的表进行查询。

#### 打开 CreateTable 类

如图所示打开 com.sequoiadb.lesson.spark.lesson2_createtable.CreateTable 类准备编写代码

![1738-420-05](https://doc.shiyanlou.com/courses/1738/1207281/cb27fd3e16482c771245d42161c77a13-0)

#### 创建关联表代码

通过 JDBC 提交建表语句关联 employee 集合

```java
// Init table
String dropTable =
        "DROP TABLE IF EXISTS employee";
// Call the method of HiveUtil to init the employee table
HiveUtil.doDDL(dropTable);
// Create a Spark table (associate with the employee collection of SequoiaDB)
String createLinkTable =
        "CREATE TABLE employee " +
                "USING com.sequoiadb.spark  " +
                "OPTIONS( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'employee', " +
                "user 'sdbadmin'," +
                "password 'sdbadmin'" +
                ")";
// Call the method of HiveUtil to execute the sql statement
System.out.println("Creating Spark table...");
HiveUtil.doDDL(createLinkTable);
// View the Spark table structure
String getDesc=
        "DESC employee";
// Call the method of HiveUtil to query the table structure
System.out.println("Getting the structure of employee...");
HiveUtil.doDQL(getDesc);
// Query the result set of Spark table
String queryTable = "SELECT id,name,sex,birth,phone,email,position,address FROM employee";
// Call the method of HiveUtil to query the result set of Spark table
System.out.println("Getting the result set of employee...");
HiveUtil.doDQL(queryTable);
```

将上述代码粘贴至 CreateTable 类 createTable 方法的 TODO -- lesson2_createtable:code1 注释处（20 行）：

![1738-420-06](https://doc.shiyanlou.com/courses/1738/1207281/c46fdd1f3b1a0f588b9226ebdb5030be-0)

## 运行程序

#### 运行主函数

右键点击 CreateTableMainTest 选择 Run 主函数

![1738-420-07](https://doc.shiyanlou.com/courses/1738/1207281/6b65def223ce0305d8ca33c855ab484c-0)

#### 运行结果

程序运行结果如下图所示，分别展示了从 MySQL 实例中 employee 表和 Hive on Spark 中 employee 关联表的结果集：

![1738-420-08](https://doc.shiyanlou.com/courses/1738/1207281/fb78215e5c42795bd3a4021100c402c9-0)

从关联表的表结构可以看出，即便在关联时没有指定表结构，Spark 也会自动为各个字段匹配相应的数据类型。

## 总结

本课程介绍了如何通过 Hive on Spark 创建和 MySQL 实例关联的外表，并通过 JDBC 的方式访问  Hive on Spark 中的关联表。在 Hive on Spark 中创建关联表时，Spark 会自动生成关联表的表结构。
