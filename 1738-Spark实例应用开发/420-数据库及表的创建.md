---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍如何通过 jdbc 的方式在 Hive on Spark 中访问外部数据源。

#### 实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* JDK version "1.8.0_172"
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* Spark version: 2.4.3
* IntelliJ IDEA Community Version: 2019.3.4

#### 知识点

在 Hive on Spark 中支持 SQL 方式关联外部数据源，只要在 Spark 中创建了外部数据源的关联表，就可以按照 JDBC 标准通过 Spark 访问外部数据源。

在 Spark 中创建关联表只需要指定数据源，无需定义表结构，在关联时 Spark 会根据结构化数据源自动生成表结构。以关联 SequoiaDB 集合空间为例，创建关联表语法及参数说明如下：

![1738-420-驱动](https://doc.shiyanlou.com/courses/1738/1207281/a040460c8cb09d8a2758b94dc284e93d-0)

> **说明**
>
> com.sequoiadb.spark 为巨杉数据库开发的 Spark 连接器，可以在 Spark 创建表时指定和 SequoiaDB 中的集合空间关联并和集合的各个数据分区做好对接。
>
> 使用连接器需要确保 `$SPARK_HOME` 的 `jars` 目录下有 `spark-sequoiadb` jar 包，并在 `spark-env.sh` 文件中配置 SPARK_CLASSPATH。在启动 thriftserver 时会自动识别 jar 包并在提交 SQL 到 Hive on Spark 时调用。

## 打开项目

#### 打开 IDEA

打开 IDEA 代码开发工具。

![1738-420-01](https://doc.shiyanlou.com/courses/1738/1207281/b478b77e961e05b3f8bfa0bf327a58ad-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-420-02](https://doc.shiyanlou.com/courses/1738/1207281/738ff5ff514f299a6dcfaf7340367c2f-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-420-03](https://doc.shiyanlou.com/courses/1738/1207281/5b3f78991e52edba44b04bfc3324bf31-0)

#### Maven 依赖

在 pom.xml 文件中可以找到当前实验中使用到的 Maven 依赖：

![1738-420-04](https://doc.shiyanlou.com/courses/1738/1207281/d2169f36c0d88c23f13ac644f3055eba-0)

## 关联 SequoiaDB 集合

程序将通过 MySQL 实例初始化 employee 表存储到 SequoiaDB 中，通过 Hive on Spark 建立和 SequoiaDB 的 employee 集合空间关联的表进行查询。

#### 打开 LinkTable 类

如图所示打开 com.sequoiadb.lesson.spark.lesson2_createtable.LinkTable 类准备编写代码

![1738-420-05](https://doc.shiyanlou.com/courses/1738/1207281/e2b6da5a3b9ac992a2e887d77eb45491-0)

#### 创建关联表代码

通过 JDBC 提交建表语句关联 employee 集合

```java
// 初始化表
String dropTable =
        "DROP TABLE IF EXISTS employee";
// 调用HiveUtil的doDDL()方法初始化employee表
HiveUtil.doDDL(dropTable);
// 创建Spark表（关联 SequoiaDB 的 employee 集合）
String createLinkTable =
        "create table employee " +
                "using com.sequoiadb.spark  " +
                "options( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'employee', " +
                "user 'sdbadmin'," +
                "password 'sdbadmin'" +
                ")";
// 调用HiveUtil的doDDL()方法执行 sql 语句
HiveUtil.doDDL(createLinkTable);
// 查看Spark表结构
String getDesc=
        "desc employee";
// 调用HiveUtil的doDQL()方法查询表结构
System.out.println("正在获取关联表结构……");
HiveUtil.doDQL(getDesc);
// 查询Spark表数据
String queryTable = "select id,name,sex,birth,phone,email,position,address from employee";
// 调用HiveUtil的doDQL()方法查询Spark表数据
System.out.println("正在获取关联表数据……");
HiveUtil.doDQL(queryTable);
```

将上述代码粘贴至 LinkTable 类 linkTable 方法的 TODO -- lesson2_createtable:code1 注释处（20 行）：

![1738-420-06](https://doc.shiyanlou.com/courses/1738/1207281/c98f9664846f9bf22c3e378e910ae855-0)

## 运行程序

#### 运行主函数

右键点击 CreateTableMainTest 选择 `Run` 运行主函数

![1738-420-07](https://doc.shiyanlou.com/courses/1738/1207281/fa8acefb45dde9bd3d11d1c8b80145ec-0)

#### 运行结果

程序运行结果如下图所示，分别展示了从 MySQL 实例中 employee 表和 Hive on Spark 中 employee 关联表的结果集：

![1738-420-08](https://doc.shiyanlou.com/courses/1738/1207281/a83dc1fc379b91c24d2e1a92ce2529f1-0)

从关联表的表结构可以看出，即便在关联时没有指定表结构，Spark 也会自动为各个字段匹配相应的数据类型。

## 总结

本课程介绍了如何通过 Hive on Spark 创建和 MySQL 实例关联的外表，并通过 jdbc 的方式访问  Hive on Spark 中的关联表。在 Hive on Spark 中创建关联表时，Spark 会自动生成关联表的表结构。
