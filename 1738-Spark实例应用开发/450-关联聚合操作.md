---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 中的 Job、Stage 和 Task 的概念，在实验中将进行 JDBC 在 Hive on Spark 中的关联与聚合操作，并通过 CREATE TABLE AS SELECT 的方式将查新创建为新表关联到 SequoiaDB 中的集合上。

####  实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* JDK version "1.8.0_172"
* IntelliJ IDEA Community Version: 2019.3.4
* spark version: 2.4.3

#### 知识点

* Job、Stage 和 Task

  第一章 已经介绍了 Spark 集群模式的工作原理，在主程序将任务分发给 Executor 后，具体任务还会被细分成为 Job、Stage 和 Task，三者之间的关系如下图所示：

  ![1738-450-01](https://doc.shiyanlou.com/courses/1738/1207281/0f064f9b7bba2fe40b2eedd04d5b8c9b-0)

  在实验中将对 Hive on Spark 提交的查询语句在 Spark UI 监控展示的信息进行解释说明。

## 打开项目

#### 打开 IDEA

打开 IDEA 代码开发工具

![1738-450-02](https://doc.shiyanlou.com/courses/1738/1207281/6e42a341e6653412eb18e192d14bda70-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-450-03](https://doc.shiyanlou.com/courses/1738/1207281/391e8241ca323719465931dec1e39f1e-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-450-04](https://doc.shiyanlou.com/courses/1738/1207281/e9547197728f1b2f47b008cbbdf48779-0)

#### Maven 依赖

在 pom.xml 文件中可以找到当前实验中使用到的 Maven 依赖

![1738-450-05](https://doc.shiyanlou.com/courses/1738/1207281/9caf84d7c768b4a9d9d745f4ed1d3d80-0)

## 准备数据

#### 打开 InitSource 类

如图所示找到初始化数据所用的 om.sequoiadb.lesson.spark.lesson5_statistic.InitSource 类：

![1738-450-06](https://doc.shiyanlou.com/courses/1738/1207281/ebc1c24e809dd758cd24a0af27949562-0)

#### 初始化源表

调用预先定义好的方法通过 MySQL 实例创建 department、salary 和 employee 表并初始化数据。代码内容如下：

```java
// 初始化数据库
MySQLUtil.initDatabase("sample");
MySQLUtil.changeDatabase("sample");
// 初始化 department 表
System.out.println("正在初始化 department 表 ……");
MySQLUtil.initTable("src/main/resources/sql/department.sql");
// 初始化 salary 表
System.out.println("正在初始化 salary 表 ……");
MySQLUtil.initTable("src/main/resources/sql/salary.sql");
// 初始化 employee 表结构
System.out.println("正在初始化 employee 表 ……");
MySQLUtil.initTable("src/main/resources/sql/employee.sql");
```

将上述代码粘贴至 InitSource 类 initTable 方法的 TODO -- lesson5_crosssource:code1 注释中（32 行）：

![1738-450-07](https://doc.shiyanlou.com/courses/1738/1207281/9be4adc25d4bf9b5e21fa1c9d1f9224d-0)

#### 创建关联表

调用预先定义好的方法创建 department、salary 和 employee 关联表。代码内容如下：

```java
// 删除已有 department 表
String dropDepartment = "drop table if exists department";
// 创建 department 关联表
String linkDepartment =
        "create table department " +
                "using com.sequoiadb.spark  " +
                "options( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'department' " +
                ")";
// 查询 department 表记录
String queryDepartment = "select * from department";
// 调用 HiveUtil 工具类执行 sql 语句
HiveUtil.doDDL(dropDepartment);
HiveUtil.doDDL(linkDepartment);
HiveUtil.doDQL(queryDepartment);
// 删除已有 salary 表
String dropSalary = "drop table if exists salary";
// 创建 salary 关联表
String linkSalary =
        "create table salary " +
                "using com.sequoiadb.spark  " +
                "options( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'salary' " +
                ")";
// 查询 salary 表记录
String querySalary="select * from salary";
// 调用 HiveUtil 工具类执行 sql 语句
HiveUtil.doDDL(dropSalary);
HiveUtil.doDDL(linkSalary);
HiveUtil.doDQL(querySalary);
// 删除已有 employee 表
String dropEmployee = "drop table if exists employee";
// 创建 employee 关联表
String linkEmployee =
        "create table employee " +
                "using com.sequoiadb.spark  " +
                "options( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'employee' " +
                ")";
// 查询 employee 表记录（部分）
String queryEmployee = "select id,name,sex,birth,phone,email,position,address from employee";
// 调用 HiveUtil 工具类执行 sql 语句
HiveUtil.doDDL(dropEmployee);
HiveUtil.doDDL(linkEmployee);
HiveUtil.doDQL(queryEmployee);
```

将上述代码粘贴至 InitSource 类 linkTables 方法的 TODO -- lesson5_crosssource:code2 注释中（21 行）：

![1738-450-08](https://doc.shiyanlou.com/courses/1738/1207281/124ae92d052fa79274d712a336c94949-0)

#### 运行程序

* 右键点击 StatisticMainTest 类选择 Create/Edit 主函数

  ![1738-450-09](https://doc.shiyanlou.com/courses/1738/1207281/ba7e05a8a7f848b88567cd7119886816-0)

* 编辑主函数的参数为 init

  ![1738-450-10](https://doc.shiyanlou.com/courses/1738/1207281/931fbb9296828f88478584e790603e27-0)

* 右键点击 StatisticMainTest 类选择 Run 主函数

  ![1738-450-11](https://doc.shiyanlou.com/courses/1738/1207281/e44895fa5214fab2bfe8221a5622cdf1-0)

* 程序运行结果如下：

  ![1738-450-12](https://doc.shiyanlou.com/courses/1738/1207281/08a5dcfb0adb1f6e178318d8bc19b986-0)

## 关联查询

#### 打开 Statistic 类

如图所示找到 com.sequoiadb.lesson.spark.lesson5_statistic.Statistic 类：

![1738-450-13](https://doc.shiyanlou.com/courses/1738/1207281/4d5ef30a5bd573a59ec9823e4d4e9cdc-0)

#### 编写关联查询代码

调用预定义的方法执行 SQL 查询各个部门以及职位的工资情况：

```java
// 创建 Spark 查询语句做两个不同数据源表的关联查询
String crossQuery =
        "select s.id,d.department,s.position,s.salary " +
                "from " +
                "department d,salary s " +
                "where s.department = d.d_id";
// 调用 HiveUtil 工具类执行 sql 语句
HiveUtil.doDQL(crossQuery);
```

将上述代码粘贴至 Statistic 类 relateQuery 方法的 TODO -- lesson5_crosssource:code3 注释中（31 行）：

![1738-450-14](https://doc.shiyanlou.com/courses/1738/1207281/024df78f40393d96dda667fbbd20d8fa-0)

#### 运行程序

* 右键点击 Statistic 类选择 Create/Edit 主函数

  ![1738-450-15](https://doc.shiyanlou.com/courses/1738/1207281/0416f13407ffcfaecfa5e5016955097f-0)

* 编辑主函数参数为 relate

  ![1738-450-16](https://doc.shiyanlou.com/courses/1738/1207281/eae9c51939c1a5dd1a0a97a6cfc0aad7-0)

* 右键点击 StatisticMainTest 类选择 Run 主函数

  ![1738-450-17](https://doc.shiyanlou.com/courses/1738/1207281/ea3ef6c5e57a53cc08cf14a118828a02-0)

* 查询结果如下

  ![1738-450-18](https://doc.shiyanlou.com/courses/1738/1207281/819ad9737def40aded500e41c842e9f7-0)

## 聚合查询

#### 打开 Statistic 类

如图所示找到 com.sequoiadb.lesson.spark.lesson5_statistic.Statistic 类：

![1738-450-19](https://doc.shiyanlou.com/courses/1738/1207281/13883168b241d7cdd00e8c89e9354a1d-0)

#### 编写聚合查询代码

调用预定义的方法执行 SQL 查询各个部门的平均工资以及职员人数并生成统计表：

```java
// 删除已有 statistic 表
String dropStatistic = "drop table if exists statistic";
// 通过 CTAS 方式创建 statistic 关联表
String linkStatistic =
    "create table statistic using com.sequoiadb.spark  " +
        "options( " +
        "host 'sdbserver1:11810', " +
        "collectionspace 'sample', " +
        "collection 'statistic' " +
        ")as( " +
        "select d.d_id,d.department,avg_table.salary_avg,avg_table.employee_num " +
        "from( " +
        "select s.department as department_id,avg(s.salary) as salary_avg,count(1) as employee_num " +
        "from employee e,salary s " +
        "where e.position = s.id " +
        "group by s.department " +
        ")as avg_table " +
        "left join department d " +
        "on avg_table.department_id = d.d_id)";
// 调用 HiveUtil 工具类执行 sql 语句
HiveUtil.doDDL(dropStatistic);
HiveUtil.doDDL(linkStatistic);
// 创建查询统计表语句
String queryStatistic = "select * from statistic";
// 调用 HiveUtil 方法查询统计表结果集
HiveUtil.doDQL(queryStatistic);
```

将上述代码粘贴至 Statistic 类 aggregateQuery 方法的 TODO -- lesson5_crosssource:code4 注释中（20 行）：

![1738-450-20](https://doc.shiyanlou.com/courses/1738/1207281/3acf4c0debb460c4c962d1e3703126b4-0)

#### 运行程序

* 右键点击 Statistic 类选择 Create/Edit 主函数

  ![1738-450-21](https://doc.shiyanlou.com/courses/1738/1207281/766c9ae079762c0558fafe8737fda9c6-0)

* 编辑主函数参数为 aggregate

  ![1738-450-22](https://doc.shiyanlou.com/courses/1738/1207281/7bf9d21a2c016c0bad869ad4d885317c-0)

* 右键点击 StatisticMainTest 类选择 Run 主函数

  ![1738-450-23](https://doc.shiyanlou.com/courses/1738/1207281/4db1c80822b818c2cb1b2dccba9ec7a4-0)

* 运行结果如下：

  ![1738-450-24](https://doc.shiyanlou.com/courses/1738/1207281/7d1fa6e9a528217eaa2e4c961afa7a24-0)


## 总结

本课程简要介绍了 Spark 任务的 Job、Stage 和 Task 概念，并且对于如何通过 JDBC 在 Hive on Spark 中的表进行关联与聚合操作进行了实验，并介绍了通过 CREATE TABLE AS SELECT 的方式将查询结果创建为新表并关联到 SequoiaDB 的集合。

