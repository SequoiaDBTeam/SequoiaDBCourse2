---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 中的 Job、Stage 和 Task 的概念，在实验中将进行 JDBC 在 Hive on Spark 中的关联与聚合操作，并通过 CREATE TABLE AS SELECT 的方式将查新创建为新表关联到 SequoiaDB 中的集合上。

####  实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* JDK version "1.8.0_172"
* IntelliJ IDEA Community Version: 2019.3.4
* spark version: 2.4.3

#### 知识点

* Job、Stage 和 Task

  第一章 已经介绍了 Spark 集群模式的工作原理，在主程序将任务分发给 Executor 后，具体任务还会被细分成为 Job、Stage 和 Task，三者之间的关系如下图所示：

  ![1738-450-01](https://doc.shiyanlou.com/courses/1738/1207281/0f064f9b7bba2fe40b2eedd04d5b8c9b-0)


## 打开项目

#### 打开 IDEA

打开 IDEA 代码开发工具

![1738-450-02](https://doc.shiyanlou.com/courses/1738/1207281/6e42a341e6653412eb18e192d14bda70-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-450-03](https://doc.shiyanlou.com/courses/1738/1207281/391e8241ca323719465931dec1e39f1e-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-450-04](https://doc.shiyanlou.com/courses/1738/1207281/1f20ee7606b07028b2039da63b491a55-0)

#### Maven 依赖

如图所示找到 pom.xml 文件：

![1738-410-pom](https://doc.shiyanlou.com/courses/1738/1207281/2096e77f8ff05283b1b51e9f5182b861-0)

在 pom.xml 文件中可以找到当前实验中使用到的 Maven 依赖

![1738-450-05](https://doc.shiyanlou.com/courses/1738/1207281/9caf84d7c768b4a9d9d745f4ed1d3d80-0)

## 关联查询

程序会自动初始化关联查询需要的数据，并在 Hive on Spark 中创建相应的关联表。下述样例为对已有的 department、salary 和 employee 表进行关联查询，获得各个部门以及职位的工资情况。

#### 打开 Aggregate 类

如图所示找到 com.sequoiadb.lesson.spark.lesson5_statistic.Aggregate 类：

![1738-450-13](https://doc.shiyanlou.com/courses/1738/1207281/a004001a1c2302b322e719817f3b87e6-0)

#### 编写关联查询代码

调用预定义的方法执行 SQL 查询各个部门以及职位的工资情况：

```java
// Create join query statement
String crossQuery =
        "SELECT s.id,d.department,s.position,s.salary " +
                "FROM " +
                "department d,salary s " +
                "WHERE s.department = d.d_id";
// Call HiveUtil to execute statement
HiveUtil.doDQL(crossQuery);
```

将上述代码粘贴至 Aggregate 类 joinQuery 方法的 TODO -- lesson5_crosssource:code1 注释中（29 行）：

![1738-450-14](https://doc.shiyanlou.com/courses/1738/1207281/6b1dd1042633f7dd98a3d925f4b8ef47-0)

#### 运行程序

* 右键点击 AggregateMainTest 类选择 Create/Edit 主函数

  ![1738-450-15](https://doc.shiyanlou.com/courses/1738/1207281/54c7f46b9a822bd6e3e22a2fcda13f27-0)

* 编辑主函数参数为 join

  ![1738-450-16](https://doc.shiyanlou.com/courses/1738/1207281/cf0283a67693f6f14d3bbfa6f9e5b3f3-0)

* 右键点击 AggregateMainTest 类选择 Run 主函数

  ![1738-450-17](https://doc.shiyanlou.com/courses/1738/1207281/bbdbe4398d8e38c8a65fa3624ceeca47-0)

* 查询结果如下

  ![1738-450-18](https://doc.shiyanlou.com/courses/1738/1207281/0efd652dcc7e288999cc990531c6232e-0)

## 聚合查询

程序会自动初始化关联查询需要的数据，并在 Hive on Spark 中创建相应的关联表。下述样例为对已有的 department、salary 和 employee 表进行聚合查询，获取各个部门的人数以及平均工资。

#### 打开 Aggregate 类

如图所示找到 com.sequoiadb.lesson.spark.lesson5_statistic.Aggregate 类：

![1738-450-13](https://doc.shiyanlou.com/courses/1738/1207281/a004001a1c2302b322e719817f3b87e6-0)

#### 编写聚合查询代码

调用预定义的方法执行 SQL 查询各个部门的平均工资以及职员人数并生成统计表：

```java
// Delete the existing statistic table
String dropStatistic = "DROP TABLE IF EXISTS statistic";
// Statement create Spark table by CTAS
String linkStatistic =
    "CREATE TABLE statistic USING com.sequoiadb.spark  " +
        "OPTIONS( " +
        "host 'sdbserver1:11810', " +
        "collectionspace 'sample', " +
        "collection 'statistic' " +
        ")AS( " +
        "SELECT d.d_id,d.department,avg_table.salary_avg,avg_table.employee_num " +
        "FROM( " +
        "SELECT s.department as department_id,avg(s.salary) AS salary_avg,count(1) AS employee_num " +
        "FROM employee e,salary s " +
        "WHERE e.position = s.id " +
        "GROUP BY s.department " +
        ")AS avg_table " +
        "LEFT JOIN department d " +
        "ON avg_table.department_id = d.d_id)";
// Call HiveUtil to execute statement
HiveUtil.doDDL(dropStatistic);
HiveUtil.doDDL(linkStatistic);
// Create statement to query result set
String queryStatistic = "SELECT * FROM statistic";
// Call HiveUtil to execute statement
HiveUtil.doDQL(queryStatistic);
```

将上述代码粘贴至 Aggregate 类 aggregateQuery 方法的 TODO -- lesson5_crosssource:code2 注释中（20 行）：

![1738-450-20](https://doc.shiyanlou.com/courses/1738/1207281/a0947fbaf9ae0da48ced970a80218bb8-0)

#### 运行程序

* 右键点击 StatisticMainTest 类选择 Create/Edit 主函数

  ![1738-450-21](https://doc.shiyanlou.com/courses/1738/1207281/9557841609c0d9c42aa6b80630127785-0)

* 编辑主函数参数为 aggregate

  ![1738-450-22](https://doc.shiyanlou.com/courses/1738/1207281/7bf9d21a2c016c0bad869ad4d885317c-0)

* 右键点击 AggregateMainTest 类选择 Run 主函数

  ![1738-450-17](https://doc.shiyanlou.com/courses/1738/1207281/bbdbe4398d8e38c8a65fa3624ceeca47-0)

* 运行结果如下：

  ![1738-450-24](https://doc.shiyanlou.com/courses/1738/1207281/bc8b97e31486d913da6efe96fe733f2f-0)


## 总结

本课程简要介绍了 Spark 任务的 Job、Stage 和 Task 概念，并且对于如何通过 JDBC 在 Hive on Spark 中的表进行关联与聚合操作进行了实验，并介绍了通过 CREATE TABLE AS SELECT 的方式将查询结果创建为新表并关联到 SequoiaDB 的集合。

