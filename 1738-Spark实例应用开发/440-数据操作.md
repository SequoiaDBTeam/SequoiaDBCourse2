---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 引擎解析 SQL 的工作机制，并通过程序实例对 Hive on Spark 的数据操作进行简要说明。

#### 实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* JDKversion "1.8.0_172"
* Spark version: 2.4.3
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* IntelliJ IDEA Community Version: 2019.3.4

#### 知识点

**Spark 计算引擎工作机制**

![1738-440-01](https://doc.shiyanlou.com/courses/1738/1207281/036f44e333e7d57a85e5327247f53dd6-0)

Spark SQL 工作机制的核心为 **Catalyst Optimizer** ，它将用户程序中的 SQL/Dataset/DataFrame 经过一系列操作，最终转化为 Spark 系统中执行的 RDD。上图 Catalyst Optimizer 的解析流程解读如下：

* 将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，之后会使用不同的 Rule 应用到该逻辑计划上

  > **说明**
  >
  > Catalyst 的 parser 不支持 update 和 delete

* Analyzer 通过一定的规则和数据元数据（SessionCatalog、Hive MetaStore）解析未绑定的逻辑计划，生成绑定的逻辑计划

* Optimizer 将绑定的逻辑计划经过一系列的合并、列裁剪和过滤器下推等操作后生成优化的逻辑计划

* Planner 依照 Planning Strategies 对优化的逻辑计划进行 Transform 生成物理可执行计划。每个 Strategy 将某个逻辑算子转化成对应的物理算子，最终会变成 RDD 的具体操作。

* 执行物理计划计算 RDD

## 打开项目

#### 打开 IDEA

打开 IDEA 代码开发工具

![1738-440-02](https://doc.shiyanlou.com/courses/1738/1207281/6526b50a5804f3670aa08ce9d22a58ed-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-440-03](https://doc.shiyanlou.com/courses/1738/1207281/b572ff7a748922a4c968dcebf9c2aff1-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-440-04](https://doc.shiyanlou.com/courses/1738/1207281/476281f8577bd49edc919f610ebb4c34-0)

#### Maven 依赖

如图所示找到 pom.xml 文件：

![1738-410-pom](https://doc.shiyanlou.com/courses/1738/1207281/2096e77f8ff05283b1b51e9f5182b861-0)

在 pom.xml 中可以找到当前实验中使用到的 Maven 依赖：

![1738-440-05](https://doc.shiyanlou.com/courses/1738/1207281/fddb0b1419a941b7a140e64c3c2f3220-0)

## 程序代码

程序将自动初始化 department 表以及表内数据，并创建 Hive 的关联表。通过 JDBC 分别对 employee 关联表进行查询、插入以及批量插入操作。

#### 打开 DataOperation 类

如图所示打开 com.sequoiadb.lesson.spark.lesson4_dataoperation.DataOperation 类：

![1738-440-06](https://doc.shiyanlou.com/courses/1738/1207281/d3e0f18569cac0f74a497adaeb1c1708-0)

#### 查询操作

创建查询 department 表全部结果集的语句，并通过 HiveUtil 类方法提交 SQL 语句。其他数据操作都会调用该查询查看数据操作效果。程序内容如下图所示：

```java
// Create the query statement
String sql = "SELECT * FROM " + tableName;
// Call HiveUtil to query the result set
HiveUtil.doDQL(sql);
```

将上述代码粘贴至 DataOperation 类 getAll 方法的 TODO -- lesson4_dataoperation:code1 注释中（43 行）：

![1738-440-07](https://doc.shiyanlou.com/courses/1738/1207281/f7559e95e6ee9f4869f9848322ff512c-0)

#### 插入操作

创建向 department 表插入记录的语句，并通过 HiveUtil 类方法提交。插入后调用 getAll 方法查看插入效果。程序内容如下图所示：

```java
// Create the insert statement
String sql = "INSERT INTO department VALUES (10,'Business')";
// Call HiveUtil to execute the insert statement
HiveUtil.doDML(sql);
// Query result
getAll("department");
```

将上述代码粘贴至 DataOperation 类 insert 方法的 TODO -- lesson4_dataoperation:code2 注释中（32 行）：

![1738-440-08](https://doc.shiyanlou.com/courses/1738/1207281/7339de0e0ec31b4c2be67a02897bcde6-0)

#### 批量插入操作

新建需要批量插入数据的目标表 new_department（和 SequoiaDB 集合关联），将 department 表中数据复制到   new_department 表中去。代码内容如下：

```java
// Initialize the new_department collection
SdbUtil.initCollection("sample","new_department");
// Delete the existing new_department table
String dropDepartment = "DROP TABLE IF EXISTS new_department";
// Create the new_department association table
// When associating with the SequoiaDB collection, users need to specify that the table structure is consistent with the select source table
String linkDepartment =
        "CREATE TABLE new_department(" +
                "d_id int," +
                "department string" +
                ") USING com.sequoiadb.spark  " +
                "OPTIONS( " +
                "host 'sdbserver1:11810', " +
                "collectionspace 'sample', " +
                "collection 'new_department' " +
                ")";
// Call the HiveUtilto execute the associated table statement
HiveUtil.doDDL(dropDepartment);
HiveUtil.doDDL(linkDepartment);
// Create the insert statement from department table
String sql = "INSERT INTO new_department SELECT * FROM department";
// Call HiveUtil to execute sql statement
HiveUtil.doDML(sql);
// Query result
getAll("new_department");
```

将上述代码粘贴至 DataOperation 类 bulkInsert 方法的 TODO -- lesson4_dataoperation:code3 注释中（21 行）：

![1738-440-09](https://doc.shiyanlou.com/courses/1738/1207281/990b489dd553de68466113a7d6915c01-0)

## 运行程序

右键点击 DataOperationMainTest 类选择 Run 主函数

![1738-440-13](https://doc.shiyanlou.com/courses/1738/1207281/8e4d2325ba066d2ac0271c0597b2fd7a-0)

运行结果如下：

![1738-440-14](https://doc.shiyanlou.com/courses/1738/1207281/0ec58b91b675c36b5f2fb6c175b20eef-0)

## 总结

通过本课程简要介绍了 Spark 计算引擎在解析并处理 Hive on Spark 提交的 SQL 时的工作机制（Catalyst Optimizer），并通过实验演示了如何使用 JDBC 操作 Hive on Spark 的数据。
