---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 引擎解析 sql 的工作机制，并通过程序实例对 Hive on Spark 支持的数据操作进行简要说明。

#### 实验环境

当前实验的系统和软件环境如下：

* Ubuntu 16.04.6 LTS
* JDKversion "1.8.0_172"
* Spark version: 2.4.3
* SequoiaDB version: 3.4
* SequoiaSQL-MySQL version: 3.4
* IntelliJ IDEA Community Version: 2019.3.4

#### 知识点

**Spark 计算引擎工作机制**

![1738-440-01](https://doc.shiyanlou.com/courses/1738/1207281/036f44e333e7d57a85e5327247f53dd6-0)

Spark SQL 工作机制的核心为 **Catalyst Optimizer** ，它将用户程序中的 SQL/Dataset/DataFrame 经过一系列操作，最终转化为 Spark 系统中执行的 RDD。上图 Catalyst Optimizer 的解析流程解读如下：

* 将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，之后会使用不同的 Rule 应用到该逻辑计划上
* Analyzer 通过一定的规则和数据元数据（SessionCatalog、Hive MetaStore）解析未绑定的逻辑计划，生成绑定的逻辑计划
* Optimizer 将绑定的逻辑计划经过一系列的合并、列裁剪和过滤器下推等操作后生成优化的逻辑计划
* Planner 依照 Planning Strategies 对优化的逻辑计划进行 Transform 生成物理可执行计划。每个 Strategy 将某个逻辑算子转化成对应的物理算子，最终会变成 RDD 的具体操作。
* 执行物理计划计算 RDD

**不支持 UPDATE/DELETE**

在第一章中我们已经介绍过 Hive on Spark 使用 Spark 作为计算引擎，会将提交给 Hive 的语句提交到 Spark 集群上进行运算处理。在上一小节中提及，Spark 在处理 SQL 时会首先将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，但 Catalyst 的 parser 是不支持 update 和 delete 的，这在实验中可以得到验证。

## 打开项目

#### 打开 IDEA

打开 IDEA 代码开发工具

![1738-440-02](https://doc.shiyanlou.com/courses/1738/1207281/6526b50a5804f3670aa08ce9d22a58ed-0)

#### 打开 SCDD-Spark 项目

选择 Spark 课程项目

![1738-440-03](https://doc.shiyanlou.com/courses/1738/1207281/b572ff7a748922a4c968dcebf9c2aff1-0)

#### 打开当前实验的 Package

如图所示找到当前实验使用的程序所在 Package

![1738-440-04](https://doc.shiyanlou.com/courses/1738/1207281/476281f8577bd49edc919f610ebb4c34-0)

#### Maven 依赖

当前实验中使用到的 Maven 依赖如下：

![1738-440-05](https://doc.shiyanlou.com/courses/1738/1207281/fddb0b1419a941b7a140e64c3c2f3220-0)

## 程序代码

程序将自动初始化 department 表以及表内数据，并创建 HIve 的关联表。通过 JDBC 分别对 employee 关联表进行查询、插入、更新和删除操作观察程序的运行结果。

#### 打开 DataOperation 类

如图所示打开 com.sequoiadb.lesson.spark.lesson4_dataoperation.DataOperation 类：

![1738-440-06](https://doc.shiyanlou.com/courses/1738/1207281/d3e0f18569cac0f74a497adaeb1c1708-0)

#### 查询操作

创建查询 department 表全部结果集的语句，并通过 HiveUtil 类方法提交 SQL 语句。其他数据操作都会调用该查询查看数据操作效果。程序内容如下图所示：

```java
// 创建查询语句
String sql = "select * from department";
// 调用 HiveUtil 类查询结果集
HiveUtil.doDQL(sql);
```

将上述代码粘贴至 DataOperation 类 getAll 方法的 `TODO -- lesson4_dataoperation:code1` 注释中（53 行）：

![1738-440-07](https://doc.shiyanlou.com/courses/1738/1207281/beeb068808281e6b1689c23c2e8c734b-0)

#### 插入操作

创建向 department 表插入记录的语句，并通过 HiveUtil 类方法提交。插入后调用 getAll 方法查看插入效果。程序内容如下图所示：

```java
// 创建插入语句
String sql = "insert into department values (10,'商务部')";
// 调用 HiveUtil 执行插入语句
HiveUtil.doDML(sql);
// 查询结果
getAll();
```

将上述代码粘贴至 DataOperation 类 insert 方法的 `TODO -- lesson4_dataoperation:code2` 注释中（42 行）：

![1738-440-08](https://doc.shiyanlou.com/courses/1738/1207281/7d99fa84739d39f67e32bf42450105ec-0)

#### 更新操作

创建更新 department 表记录的语句，并通过 HiveUtil 类方法提交。更新后调用 getAll 方法查看更新效果。程序内容如下图所示：

```java
// 创建更新语句
String sql = "update department set departmenmt = '宣传部' where id = 1";
// 调用 HiveUtil 类执行更新语句
HiveUtil.doDML(sql);
// 查询结果
getAll();
```

将上述代码粘贴至 DataOperation 类 update 方法的 `TODO -- lesson4_dataoperation:code3` 注释中（31 行）：

![1738-440-09](https://doc.shiyanlou.com/courses/1738/1207281/292e907898d8c41a9185b9d1ed4b2812-0)

#### 删除操作

创建删除 department 表记录的语句，并通过 HiveUtil 类方法提交。删除后调用 getAll 方法查看更新效果。程序内容如下图所示：

```java
// 创建删除语句
String sql = "delete from department where id = 1";
// 调用 HiveUtil 类执行删除语句
HiveUtil.doDML(sql);
// 查询结果
getAll();
```

将上述代码粘贴至 DataOperation 类 delete 方法的 `TODO -- lesson4_dataoperation:code4` 注释中（20 行）：

![1738-440-10](https://doc.shiyanlou.com/courses/1738/1207281/f1d6663c273b06a81faaf168f19ca052-0)

## 运行程序

#### 编辑主类参数

右键点击 DataOperationMainTest 类选择 `Create` (或`Edit`) 主函数

![1738-440-11](https://doc.shiyanlou.com/courses/1738/1207281/5b49c5e08fb1eb83a2aee7b5a29e0e03-0)

#### 插入操作

编辑主函数参数为 `insert` 

![1738-440-12](https://doc.shiyanlou.com/courses/1738/1207281/c20d44e394d6fc959917eb3e830706b9-0)

右键点击 DataOperationMainTest 类选择 `Run` 主函数

![1738-440-13](https://doc.shiyanlou.com/courses/1738/1207281/8e4d2325ba066d2ac0271c0597b2fd7a-0)

运行结果如下：

![1738-440-14](https://doc.shiyanlou.com/courses/1738/1207281/0a769022a6060fb260b658850d63efc4-0)

#### 更新操作

参照 *插入操作* 中的运行方式，编辑主函数参数为 `update`，运行结果如下：

![1738-440-15](https://doc.shiyanlou.com/courses/1738/1207281/1f9c2e751cfe4e1ee14b2b332dc9631d-0)

#### 删除操作

参照 *插入操作* 中的运行方式，编辑主函数参数为 `delete`，运行结果如下：

![1738-440-16](https://doc.shiyanlou.com/courses/1738/1207281/7478bdd56b66d5f839e829d785faa329-0)

可以看到，由于 Spark 的 Catalyst 优化器并不能解析 update 和 delete 词法，update 和 delete 操作都抛出了 *org.apache.spark.sql.catalyst.parser.ParseException* 。

## 5 总结

通过本课程，可以初步了解 Spark 计算引擎在解析并处理 Hive on Spark 提交的 SQL 时的工作机制（Catalyst Optimizer），并通过实验了解并验证了 Spark 并不支持 SQL 的 update 和 delete 操作。
